# 3. Replace Postgres RDB with JSON

Date: 2019-02-07

## Status

Accepted

## Context

A point of repeated discussion within the development team has been whether or not a relational database is the most appropriate persistence option for the Author product.

The GraphQL specification and the documentation of associated libraries do not prescribe any one best method for the persistence of data. Rather, they suggest thinking of GraphQL as an abstraction layer that can be wrapped around any type of backend persistence.

In the early stages of development on the Author product, a decision was taken to start with a relational database, which has served us well until this point.

Over time, the database specific code in the API has added to the complexity of the API and have made some requirements more difficult to implement than they ought to be. Examples of such features include Duplication, Moving and Ordering.

The team has some new requirements to implement a permissions model around Author's questionnaire data. Due to the the relational nature of our data model, we estimate that in order check if an author has permissions to perform a mutation at one of the lower levels of the relational tree (say, update a validation rule) several joins would need to happen just to get back up to the root of the data model (i.e. Questionnaire) just so that permissions could be checked.

Moreover, this check would likely need to happen on every request, resulting in increased complexity.

Another motivating factor for revisiting our persistence mechanism is our current inability to easily port questionnaires between environments. 

At present, there isn't an easy way of taking a questionnaire built in one environment e.g. pre-prod and migrating that data into another environment e.g. local/dev/test/prod. There are a few reasons for this, but one of the primary reasons is that database autogenerated Ids make it more difficult to ingest data from other environments and data would need to be migrated from many tables to do this successfully.

### Motivations

We hope making this change will make it:

- Easier to make changes to the API going forward, as there is less complexity and less code to maintain.
- Easier to write tests (most functionality can be tested by unit tests on pure functions).
- Easier to move data between environments (for example from pre-prod to prod)
- Easier to implement permissions checking.
- Easier to keep versioned history of changes and restore to point in time.
- Cheaper to run cloud infrastructure (serverless), and overall more performant.
- More portable if we need to migrate to another cloud provider.
- Require significantly less calls to the datastore per request.

We also believe that we will see improvements to our CI and Cypress test execution times since it will be possible to load in a questionnaire in a pre-configured state rather than having to build up scenario in each test.

## Decision

The decision taken is to refactor the API to use a JSON document database as a persistence mechanism.

Initially our implementation will be focused on targeting DynamoDB, with the ability to run the application locally using either an in-memory or containerised local DynamoDB instance. However we're striving to not become dependent on any one cloud provider and so we should have a very thin layer of integration with Dynamo and rely on reading and mutating an in-memory JSON document for the most part.

## Consequences

### Benefits of making the change

We believe that we will see benefits/improvements in the following areas after making this change:

- Performance improvements (single read operation for queries, single write operation for mutations)
- Simplified duplication logic.
- Simplified ordering/moving logic.
- Simplified author validation logic (we could leverage JSON schema validation for some of it).
- Simplified code base (can probably delete most of the API code (repositories, data entities, migrations etc)
- Since we would no longer rely on numeric autogenerated Ids from the database it will allow us to provide human-readable URLs which has been noted as being important for analytics.
- We will have the ability to support point-in-time restore of data by assigning each schema change a new version.

### Risks and mitgations

This is a large change to the API and so the main risk to delivery is ensuring that no functionality and/or data is lost as part of this change.

We plan on mitigating this risk by ensuring that we keep a high level of test coverage, particularly around the API which is it main contract between the backend and the UI and ensuring that we test data migrations thoroughly in a dev environment before switching over to this new mechanism in our pre-prod and prod environments.

The team aims to unblock other development tasks as soon as possible, so we will seek to merge code back into master as soon as it is stable and has sufficient test coverage. We expect that some of the code will be sub-optimal at the beginning, but are committed to improving this over time with subsequent pull requests.
